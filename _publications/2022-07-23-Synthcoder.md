---
title: "Synthetic Data is All You Need? Towards Universal Neural Vocoding"
collection: publications
permalink: /publication/2022-Synthcoder
excerpt: ''
date: 2022-07-23
venue: 'under review in INTERSPEECH'
paperurl: ''
citation: 'Zilin Wang, Peng Liu, <b>Jun Chen</b>, Zhiyong Wu, Chao Weng, Dan Su, Helen Meng. &quot;Synthetic Data is All You Need? Towards Universal Neural Vocoding&quot;. <i>Under review in INTERSPEECH 2022</i>.'

---
Abstract
===
A very intuitive idea to construct a universal vocoder is covering all possible human voices in a training database. Unfortunately, it is impossible in practice owing to the massive human and material resources involved and the copyright issues. In this work, we propose a novel and effective paradigm for training the universal vocoder with synthetic data alone, which is easily available in bulk. Our goal is to make the synthetic data distribution approximate the real vocal data distribution by enumerating synthetic audios with possible human voice forms. By this means, the universal vocoder can cope with many human voice synthesis scenarios including speech, singing voice, etc. To evaluate the universality of the vocoder trained with synthetic data, which is referred to as synthcoder in this paper, we performed experiments on both speech and singing utterances in subjective and objective metrics. The experimental results show that, as a method that does not require human audio corpus for training, synthcoder can achieve competitive results, which demonstrates the huge potential of our proposed method.